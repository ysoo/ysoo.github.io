[{"content":"The Goal The goal is to learn about Sentiment Analysis and how to perform it. As part of the fellowship.ai challenge, I want to write down what I have learnt through this process and what I can do to further learn more. The prompt in the challenge is really vague, with just a link to the IMDB Dataset that contains 50k movie reviews and that I should perform Sentiment Analysis on it.\nCurrent State Of The Art (September 2024) Data pulled from NLP Progress.\n* Side note, NLP Progress is amazing but why don\u0026rsquo;t we have something equivalent for other areas of deep learning? Say Computer Vision?\nWhat is Sentiment Analysis? Sentiment Analysis is the task of classifying the polarity of given text.\nI have, in the past, at a hackathon with some friends, scraped public data from Reddit and Twitter that contains mentions of publicly traded companies and performed sentiment analysis (through the Google API at the time) on them. We used that data and corroborated with how the stock performed that day on the markets to signal whether or not the stock is a buy or a sell that day.\nThe Strategy I think I would like to start with how sentiment analysis has evolved throughout the years. And I would also like to compare and contrast the different start of the art models and learn how they work\nThe dataset that I will be using would be the IMDB dataset. It contains 50k reviews and are equally split between positive and negative ones. Negative review scores are \u0026lt;= 4 and positive reviews are \u0026gt;= 7. No more than 30 reviews are included per movie.\nThroughout the years For simplicity, I have asked ChatGPT to summarize the improvements in sentiment analysis throughout the decades.\n2013-2014: Traditional ML models like Naive Bayes and SVM, with lexicon-based methods, dominated sentiment analysis. 2015-2016: Word2Vec and GloVe embeddings, along with early (Recurrent Neural Networks) RNNs and (Long Short-Term Memory) LSTMs, improved sentiment analysis by capturing semantic relationships. 2017-2018: Attention mechanisms, ELMo, and ULMFiT introduced better context handling and transfer learning to sentiment analysis. 2019-2020: BERT and Transformer models revolutionized sentiment analysis with bidirectional context understanding and fine-tuning. 2021-2022: GPT-3 and multilingual models like XLM-R enabled few-shot and zero-shot learning in sentiment analysis. 2023-Present: Sentiment analysis reached human-level accuracy with models like GPT-3 excelling in few-shot, zero-shot learning, and multimodal data analysis. In this series, I think I might omit traditional ML models. While very interesting and is the basis for a lot of the information that we have today, I think my interest lies more in deep learning (at the moment) and less so on classic machine learning.\nWhat are Recurrent Neural Networks? I think this article assumes that we know what recurrent neural networks are. I think a pretty good learning resource for this is Ava and Alexander Amini\u0026rsquo;s lecture series on YouTube. While I think the lecture on recurrent neural network itself wasn\u0026rsquo;t the best out of the lecture series, it gave me a good idea on what it is. In fact, nearly all the graphics in this article are pulled from their lecture series.\nThe way that I think about Recurrent Neural Networks is that it is a regular feed forward neural network, but with the addition of the time parameter. It is used for sequential data, aka, speech, language, time serieses.\nTo do that, it loops on itself, aka, recurrence. It has a bunch of states for each step in the sequence that are hidden.\nChallenges of the Recurrent Neural Network Exploding Gradient Problem\nWhat are Long Short-Term Memory? Maintain a cell state Use gates to control the flow of information forget gate gets ride of irrelevant information store relevant information from current input selectively update cell state output gate returns a filtered version of the cell state Backpropagation through time with partially uninterrupted gradient flow What are Word Embeddings? For RNNs, to adapt them for usage in natural language, we need to have a way to represent words in the form that a neural network would understand. Word embeddings are one way of doing them in a way that captures semantic relationships between words.\nFor example:\n{ \u0026#34;dog\u0026#34;: 1 \u0026#34;cat\u0026#34;: 2 \u0026#34;apple\u0026#34;: 30 -\u0026gt; since apple is rarely used with dog or cat } Examples of some embeddings\nWord2Vec GloVe What are Attention Mechanisms? Recurrence is expensive. To solve it, we only take into consideration what is deserving of our attention. The goal of attention mechanisms are to identify which parts within the input to attend to and extract the features with high attention.\nEncode position information Extract query, key, value for search Compute attention weighting Take the query and key vectors, and compute pairwise similarity (using cosine similarity) Extract features with high attention * *Andrej Kaparthy\u0026rsquo;s lecture series on creating makemore has been helpful to me in understanding this\nCoVe (Context Vectors)\nOriginally trained for machine translating tasks Contextualizled word embeddings that are pre-trained on machine translation ElMo (Embeddings from Language Models)\nContextualized Word Embeddings bidirectional LSTM Network to create word embeddings Transfer Learning ULMFiT (Universal Language Model Fine-tuning)\nTransfer Learning What are Transformers Recurrent neural network that are built on attention. It is essentially, multiple attention mechanisms that are put together.\nBERT\nTransformer-based model pre-trained on large corpora Uses a bidirectional approach to understand context GPT\nTransformer-based model Autoregressive model -\u0026gt; meaning it generates text by predicting the next word (or token) based on the previous context. (in comparison to BERT which is a bidirectional model) What is few-shot and zero-shot learning? Essentially, the ability for pre trained models are used for different purposes and instead of training those models for the purposes that we intend for them to do, we are not giving them any task specific information.\n","permalink":"https://ysoo.github.io/posts/sentiment-analysis/part1/","summary":"\u003ch3 id=\"the-goal\"\u003eThe Goal\u003c/h3\u003e\n\u003cp\u003eThe goal is to learn about \u003cstrong\u003eSentiment Analysis\u003c/strong\u003e and how to perform it. As part of the fellowship.ai challenge, I want to write down what I have learnt through this process and what I can do to further learn more. The prompt in the challenge is really vague, with just a link to the IMDB Dataset that contains 50k movie reviews and that I should perform Sentiment Analysis on it.\u003c/p\u003e","title":"Sentiment Analysis: Part 1"},{"content":"This is the first post in this.\nI want to make a concious effort to write more and write better. Especially as I learn throughout my life and career, I have lost a lot of information to this brain of mine, who can\u0026rsquo;t seem to hold on to anything.\nI have a few grand aunts that happen to have Alzheimers. While 23andme claim that it doesn\u0026rsquo;t run in my genes. With the rate that I don\u0026rsquo;t remember things. I worry that one day I would lose all the collective and precious knowledge that comes in this noggin of mine.\nIn addition, I find that writing things down helps with the processing of information and emotions. The act of writing things down itself already helps with sending that knowledge into the hard drive of my brain instead of storing it in RAM. Plus, with the world of large language models, I don\u0026rsquo;t even know why I would need to struggle at all in forming coherent paragraphs and sentences.\nAnyway. I am hoping to start a habit of writing frequently.\n","permalink":"https://ysoo.github.io/posts/hello-world/","summary":"\u003cp\u003eThis is the first post in this.\u003c/p\u003e\n\u003cp\u003eI want to make a concious effort to write more and write better. Especially as I learn throughout my life and career, I have lost a lot of information to this brain of mine, who can\u0026rsquo;t seem to hold on to anything.\u003c/p\u003e\n\u003cp\u003eI have a few grand aunts that happen to have Alzheimers. While 23andme claim that it doesn\u0026rsquo;t run in my genes. With the rate that I don\u0026rsquo;t remember things. I worry that one day I would lose all the collective and precious knowledge that comes in this noggin of mine.\u003c/p\u003e","title":"Hello World"}]